---
title: "Extreme gradient boosting time series forecasting"
author: "Peter Ellis"
date: "6 November 2016"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Extreme gradient boosting time series forecasting}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


Hi there.


## Tourism forecasting competition
Here is a more substantive example.  I use the 1,311 datasets from the 2010 Tourism Forecasting Competition described in
 in [Athanasopoulos et al (2011)](http://robjhyndman.com/papers/forecompijf.pdf), originally in the International Journal of Forecasting (2011) 27(3), 822-844.  The data are available in the CRAN package [Tcomp](https://cran.r-project.org/package=Tcomp).  Each data object is a list, with elements inlcuding `x` (the original training data), `h` (the forecasting period) and `xx` (the test data of length `h`).  Only univariate time series are included.
 
To give the `xgbts` model a good test, I am going to compare its performance in forecasting the 1,311 `xx` time series from the matching `x` series with three other modelling approaches:

- Auto-regressive integrated moving average (ARIMA)
- Theta
- Neural networks

Those three are all from Rob Hyndman's `forecast` package.  I am also going to look at the performance of ensembles of the four model types.  With all combinations this means 15 models in total.

Because all four models use the `forecast` paradigm it is relatively straightforward to structure the analysis.  The code below is a little repetitive but should be fairly transparent.  Because of the scale and the embarrassingly parallel nature of the work (ie no particular reason to do it in any particular order, so easy to split into tasks for different processes to do in parallel), I use `foreach` and `doParallel` to make the best use of my 8 logical processors.  The code below sets up a cluster for the parallel computing and a function `competition` which will work on any object of class `Mcomp`, which `Tcomp` inherits from the `Mcomp` package providing the first three "M" forecasting competition data collections.

```{r message = FALSE}
#=============prep======================
library(Tcomp)
library(foreach)
library(doParallel)
library(forecastxgb)
library(dplyr)
library(ggplot2)
library(scales)
```
```{r eval = FALSE}
#============set up cluster for parallel computing===========
cluster <- makeCluster(7) # only any good if you have at least 7 processors :)
registerDoParallel(cluster)

clusterEvalQ(cluster, {
  library(Tcomp)
  library(forecastxgb)
})


#===============the actual analytical function==============
competition <- function(collection, maxfors = length(collection)){
  if(class(collection) != "Mcomp"){
    stop("This function only works on objects of class Mcomp, eg from the Mcomp or Tcomp packages.")
  }
  nseries <- length(collection)
  mases <- foreach(i = 1:maxfors, .combine = "rbind") %dopar% {
    thedata <- collection[[i]]  
    mod1 <- xgbts(thedata$x)
    fc1 <- forecast(mod1, h = thedata$h)
    fc2 <- thetaf(thedata$x, h = thedata$h)
    fc3 <- forecast(auto.arima(thedata$x), h = thedata$h)
    fc4 <- forecast(nnetar(thedata$x), h = thedata$h)
    # copy the skeleton of fc1 over for ensembles:
    fc12 <- fc13 <- fc14 <- fc23 <- fc24 <- fc34 <- fc123 <- fc124 <- fc134 <- fc234 <- fc1234 <- fc1
    # replace the point forecasts with averages of member forecasts:
    fc12$mean <- (fc1$mean + fc2$mean) / 2
    fc13$mean <- (fc1$mean + fc3$mean) / 2
    fc14$mean <- (fc1$mean + fc4$mean) / 2
    fc23$mean <- (fc2$mean + fc3$mean) / 2
    fc24$mean <- (fc2$mean + fc4$mean) / 2
    fc34$mean <- (fc3$mean + fc4$mean) / 2
    fc123$mean <- (fc1$mean + fc2$mean + fc3$mean) / 3
    fc124$mean <- (fc1$mean + fc2$mean + fc4$mean) / 3
    fc134$mean <- (fc1$mean + fc3$mean + fc4$mean) / 3
    fc234$mean <- (fc2$mean + fc3$mean + fc4$mean) / 3
    fc1234$mean <- (fc1$mean + fc2$mean + fc3$mean + fc4$mean) / 4
    mase <- c(accuracy(fc1, thedata$xx)[2, 6],
              accuracy(fc2, thedata$xx)[2, 6],
              accuracy(fc3, thedata$xx)[2, 6],
              accuracy(fc4, thedata$xx)[2, 6],
              accuracy(fc12, thedata$xx)[2, 6],
              accuracy(fc13, thedata$xx)[2, 6],
              accuracy(fc14, thedata$xx)[2, 6],
              accuracy(fc23, thedata$xx)[2, 6],
              accuracy(fc24, thedata$xx)[2, 6],
              accuracy(fc34, thedata$xx)[2, 6],
              accuracy(fc123, thedata$xx)[2, 6],
              accuracy(fc124, thedata$xx)[2, 6],
              accuracy(fc134, thedata$xx)[2, 6],
              accuracy(fc234, thedata$xx)[2, 6],
              accuracy(fc1234, thedata$xx)[2, 6])
    mase
  }
  message("Finished fitting models")
  colnames(mases) <- c("x", "f", "a", "n", "xf", "xa", "xn", "fa", "fn", "an",
                        "xfa", "xfn", "xan", "fan", "xfan")
  return(mases)
}
```

Applying this function to the three different subsets of tourism data (by different frequency) is straightforward but takes a few minutes to run:

```{r eval = FALSE}
#========Fit models==============
system.time(t1  <- competition(subset(tourism, "yearly")))
system.time(t4 <- competition(subset(tourism, "quarterly")))
system.time(t12 <- competition(subset(tourism, "monthly")))

# shut down cluster to avoid any mess:
stopCluster(cluster)
```

The `competition` function returns the mean absolute scaled error (MASE) of every model combination for every dataset.  The following code creates a summary object from the objects `t1`, `t4` and `t12` that hold those individual results:

```{r eval = FALSE}
#==============present results================
results <- c(apply(t1, 2, mean),
             apply(t4, 2, mean),
             apply(t12, 2, mean))

results_df <- data.frame(MASE = results)
results_df$model <- as.character(names(results))
periods <- c("Annual", "Quarterly", "Monthly")
results_df$Frequency <- rep.int(periods, times = c(15, 15, 15))

best <- results_df %>%
  group_by(model) %>%
  summarise(MASE = mean(MASE)) %>%
  arrange(MASE) %>%
  mutate(Frequency = "Average")

Tcomp_results <- results_df %>%
  rbind(best) %>%
  mutate(model = factor(model, levels = best$model)) %>%
  mutate(Frequency = factor(Frequency, levels = c("Annual", "Average", "Quarterly", "Monthly")))
```

The resulting object, `Tcomp_results`, is provided with the `forecastxgb` package.  Visual inspection shows that the average values of MASE provided for the Theta and ARIMA models match those in the [`Tcomp` vignette](https://cran.r-project.org/web/packages/Tcomp/vignettes/tourism-comp.html).  The results are easiest to understand graphically.

```{r, fig.width = 8, fig.height = 6}
leg <- "f: Theta; forecast::thetaf\na: ARIMA; forecast::auto.arima
n: Neural network; forecast::nnetar\nx: Extreme gradient boosting; forecastxgb::xgbts"

Tcomp_results %>%
  ggplot(aes(x = model, y =  MASE, colour = Frequency, label = model)) +
  geom_text(size = 4) +
  geom_line(aes(x = as.numeric(model)), alpha = 0.25) +
  scale_y_continuous("Mean scaled absolute error\n(smaller numbers are better)") +
  annotate("text", x = 2, y = 3.5, label = leg, hjust = 0) +
  ggtitle("Average error of four different timeseries forecasting methods\n2010 Tourism Forecasting Competition data") +
  labs(x = "Model, or ensemble of models\n(further to the left means better overall performance)") +
  theme_grey(9)
```


We see the overall best performing ensemble is the average of the Theta and ARIMA models - the two from the more traditional timeseries forecasting approach.  The two machine learning methods (neural network and extreme gradient boosting) are not as effective, at least in these implementations.  As individual methods, they are the two weakest, although the extreme gradient boosting method provided in `forecastxgb` performs noticeably better than `forecast::nnetar`.

In combination with