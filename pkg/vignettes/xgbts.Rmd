---
title: "Extreme gradient boosting time series forecasting"
author: "Peter Ellis"
date: "6 November 2016"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Extreme gradient boosting time series forecasting}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The `forecastxgb` package provides time series modelling and forecasting functions, combining the machine learning approach of Chen, He and Benesty's [`xgboost` package](https://CRAN.R-project.org/package=xgboost) with the convenient handling of time series and familiar API of Rob Hyndman's [`forecast` package](http://github.com/robjhyndman/forecast).

## Basic usage

The workhorse function is `xgbts`.  This fits a model to a time series.  It creates a matrix of explanatory variables based on lagged versions of the response time series, dummy variables for seasons, and numeric time. 
```{r echo = FALSE}
set.seed(123)
```

### Univariate

Usage with default values is straightforward.  Here it is fit to Australian monthly gas production 1956-1995, an example dataset provided in `forecast`:
```{r message = FALSE}
library(forecastxgb)
model <- xgbts(gas)
```
By default, `xgbts` uses row-wise cross-validation to determine the best number of rounds of iterations for the boosting algorithm without overfitting.  A final model is then fit on the full available dataset.  The relative importance of the various features in the model can be inspected by `importance_xgb()` or, more conveniently, the `summary` method for objects of class `xgbts`.


```{r}
summary(model)
```
We see in the case of the gas data that the most important feature in explaining gas production is the production 12 months previously; and then other features decrease in importance from there but still have an impact.

Forecasting is the main purpose of this package, and a `forecast` method is supplied.  The resulting objects are of class `forecast` and familiar generic functions work with them.

```{r}
fc <- forecast(model, h = 12)
plot(fc)
```
Note that prediction intervals are not currently available.

### With external regressors
External regressors can be added by using the `xreg` argument familiar from other forecast functions like `auto.arima` and `nnetar`.

## Advanced usage

## Tourism forecasting competition
Here is a more substantive example.  I use the 1,311 datasets from the 2010 Tourism Forecasting Competition described in
 in [Athanasopoulos et al (2011)](http://robjhyndman.com/papers/forecompijf.pdf), originally in the International Journal of Forecasting (2011) 27(3), 822-844.  The data are available in the CRAN package [Tcomp](https://cran.r-project.org/package=Tcomp).  Each data object is a list, with elements inlcuding `x` (the original training data), `h` (the forecasting period) and `xx` (the test data of length `h`).  Only univariate time series are included.
 
To give the `xgbts` model a good test, I am going to compare its performance in forecasting the 1,311 `xx` time series from the matching `x` series with three other modelling approaches:

- Auto-regressive integrated moving average (ARIMA)
- Theta
- Neural networks

Those three are all from Rob Hyndman's `forecast` package.  I am also going to look at the performance of ensembles of the four model types.  With all combinations this means 15 models in total.

Because all four models use the `forecast` paradigm it is relatively straightforward to structure the analysis.  The code below is a little repetitive but should be fairly transparent.  Because of the scale and the embarrassingly parallel nature of the work (ie no particular reason to do it in any particular order, so easy to split into tasks for different processes to do in parallel), I use `foreach` and `doParallel` to make the best use of my 8 logical processors.  The code below sets up a cluster for the parallel computing and a function `competition` which will work on any object of class `Mcomp`, which `Tcomp` inherits from the `Mcomp` package providing the first three "M" forecasting competition data collections.

```{r message = FALSE}
#=============prep======================
library(Tcomp)
library(foreach)
library(doParallel)
library(forecastxgb)
library(dplyr)
library(ggplot2)
library(scales)
```
```{r eval = FALSE}
#============set up cluster for parallel computing===========
cluster <- makeCluster(7) # only any good if you have at least 7 processors :)
registerDoParallel(cluster)

clusterEvalQ(cluster, {
  library(Tcomp)
  library(forecastxgb)
})


#===============the actual analytical function==============
competition <- function(collection, maxfors = length(collection)){
  if(class(collection) != "Mcomp"){
    stop("This function only works on objects of class Mcomp, eg from the Mcomp or Tcomp packages.")
  }
  nseries <- length(collection)
  mases <- foreach(i = 1:maxfors, .combine = "rbind") %dopar% {
    thedata <- collection[[i]]  
    mod1 <- xgbts(thedata$x)
    fc1 <- forecast(mod1, h = thedata$h)
    fc2 <- thetaf(thedata$x, h = thedata$h)
    fc3 <- forecast(auto.arima(thedata$x), h = thedata$h)
    fc4 <- forecast(nnetar(thedata$x), h = thedata$h)
    # copy the skeleton of fc1 over for ensembles:
    fc12 <- fc13 <- fc14 <- fc23 <- fc24 <- fc34 <- fc123 <- fc124 <- fc134 <- fc234 <- fc1234 <- fc1
    # replace the point forecasts with averages of member forecasts:
    fc12$mean <- (fc1$mean + fc2$mean) / 2
    fc13$mean <- (fc1$mean + fc3$mean) / 2
    fc14$mean <- (fc1$mean + fc4$mean) / 2
    fc23$mean <- (fc2$mean + fc3$mean) / 2
    fc24$mean <- (fc2$mean + fc4$mean) / 2
    fc34$mean <- (fc3$mean + fc4$mean) / 2
    fc123$mean <- (fc1$mean + fc2$mean + fc3$mean) / 3
    fc124$mean <- (fc1$mean + fc2$mean + fc4$mean) / 3
    fc134$mean <- (fc1$mean + fc3$mean + fc4$mean) / 3
    fc234$mean <- (fc2$mean + fc3$mean + fc4$mean) / 3
    fc1234$mean <- (fc1$mean + fc2$mean + fc3$mean + fc4$mean) / 4
    mase <- c(accuracy(fc1, thedata$xx)[2, 6],
              accuracy(fc2, thedata$xx)[2, 6],
              accuracy(fc3, thedata$xx)[2, 6],
              accuracy(fc4, thedata$xx)[2, 6],
              accuracy(fc12, thedata$xx)[2, 6],
              accuracy(fc13, thedata$xx)[2, 6],
              accuracy(fc14, thedata$xx)[2, 6],
              accuracy(fc23, thedata$xx)[2, 6],
              accuracy(fc24, thedata$xx)[2, 6],
              accuracy(fc34, thedata$xx)[2, 6],
              accuracy(fc123, thedata$xx)[2, 6],
              accuracy(fc124, thedata$xx)[2, 6],
              accuracy(fc134, thedata$xx)[2, 6],
              accuracy(fc234, thedata$xx)[2, 6],
              accuracy(fc1234, thedata$xx)[2, 6])
    mase
  }
  message("Finished fitting models")
  colnames(mases) <- c("x", "f", "a", "n", "xf", "xa", "xn", "fa", "fn", "an",
                        "xfa", "xfn", "xan", "fan", "xfan")
  return(mases)
}
```

Applying this function to the three different subsets of tourism data (by different frequency) is straightforward but takes a few minutes to run:

```{r eval = FALSE}
#========Fit models==============
system.time(t1  <- competition(subset(tourism, "yearly")))
system.time(t4 <- competition(subset(tourism, "quarterly")))
system.time(t12 <- competition(subset(tourism, "monthly")))

# shut down cluster to avoid any mess:
stopCluster(cluster)
```

The `competition` function returns the mean absolute scaled error (MASE) of every model combination for every dataset.  The following code creates a summary object from the objects `t1`, `t4` and `t12` that hold those individual results:

```{r eval = FALSE}
#==============present results================
results <- c(apply(t1, 2, mean),
             apply(t4, 2, mean),
             apply(t12, 2, mean))

results_df <- data.frame(MASE = results)
results_df$model <- as.character(names(results))
periods <- c("Annual", "Quarterly", "Monthly")
results_df$Frequency <- rep.int(periods, times = c(15, 15, 15))

best <- results_df %>%
  group_by(model) %>%
  summarise(MASE = mean(MASE)) %>%
  arrange(MASE) %>%
  mutate(Frequency = "Average")

Tcomp_results <- results_df %>%
  rbind(best) %>%
  mutate(model = factor(model, levels = best$model)) %>%
  mutate(Frequency = factor(Frequency, levels = c("Annual", "Average", "Quarterly", "Monthly")))
```

The resulting object, `Tcomp_results`, is provided with the `forecastxgb` package.  Visual inspection shows that the average values of MASE provided for the Theta and ARIMA models match those in the [`Tcomp` vignette](https://cran.r-project.org/web/packages/Tcomp/vignettes/tourism-comp.html).  The results are easiest to understand graphically.

```{r, fig.width = 8, fig.height = 6}
leg <- "f: Theta; forecast::thetaf\na: ARIMA; forecast::auto.arima
n: Neural network; forecast::nnetar\nx: Extreme gradient boosting; forecastxgb::xgbts"

Tcomp_results %>%
  ggplot(aes(x = model, y =  MASE, colour = Frequency, label = model)) +
  geom_text(size = 4) +
  geom_line(aes(x = as.numeric(model)), alpha = 0.25) +
  scale_y_continuous("Mean scaled absolute error\n(smaller numbers are better)") +
  annotate("text", x = 2, y = 3.5, label = leg, hjust = 0) +
  ggtitle("Average error of four different timeseries forecasting methods\n2010 Tourism Forecasting Competition data") +
  labs(x = "Model, or ensemble of models\n(further to the left means better overall performance)") +
  theme_grey(9)
```


We see the overall best performing ensemble is the average of the Theta and ARIMA models - the two from the more traditional timeseries forecasting approach.  The two machine learning methods (neural network and extreme gradient boosting) are not as effective, at least in these implementations.  As individual methods, they are the two weakest, although the extreme gradient boosting method provided in `forecastxgb` performs noticeably better than `forecast::nnetar`.

Theta by itself is the best performing with the annual data - simple methods work well when the dataset is small and highly aggregate.  The best that can be said of the `xgbts` approach in this context is that it doesn't damage the Theta method much when included in a combination - several of the better performing ensembles have `xgbts` as one of their members.  In contrast, the neural network models do badly with this collection of annual data.

Adding `auto.arima` and `xgbts` to an ensemble of quarterly or monthly data definitely improves on Theta by itself.  The best performing single model for quarterly or monthly data is `auto.arima` followed by `thetaf`.  Again, neural networks are the poorest of the four individual models.

Overall, I conclude that with univariate data, `xgbts` has little to add to an ensemble that already contains `auto.arima` and `thetaf` (or - not shown - the closely related `ets`).  I believe however that inclusion of `xreg` external regressors would shift the balance in favour of `xgbts` and maybe even `nnetar` - the more complex and larger the dataset, the better the chance that these methods will have something to offer.  If and when I find a large collection of timeseries competition data with external regressors I will probably add a second vignette, or at least a blog post at [http://ellisp.github.io](http://ellisp.github.io).